name: Data Scraper - 自动数据更新

on:
  # 每天UTC时间 6:00 运行 (美国东部时间 1:00 AM)
  schedule:
    - cron: '0 6 * * *'
  
  # 允许手动触发
  workflow_dispatch:
    inputs:
      scraper_type:
        description: '选择要运行的爬虫'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - chicago
          - nyc
          - la
          - federal

jobs:
  scrape-data:
    runs-on: ubuntu-latest
    
    env:
      NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      OPENCORPORATES_API_KEY: ${{ secrets.OPENCORPORATES_API_KEY }}
    
    steps:
      - name: 检出代码
        uses: actions/checkout@v4
      
      - name: 设置 Python 环境
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'python-scraper/requirements.txt'
      
      - name: 安装依赖
        run: |
          cd python-scraper
          pip install -r requirements.txt
      
      - name: 运行数据爬虫
        run: |
          cd python-scraper
          python main_scraper.py
        env:
          SCRAPER_TYPE: ${{ github.event.inputs.scraper_type || 'all' }}
      
      - name: 上传日志
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_id }}
          path: python-scraper/logs/
          retention-days: 7
      
      - name: 发送通知 (成功)
        if: success()
        run: |
          echo "数据爬取成功完成于 $(date)"
      
      - name: 发送通知 (失败)
        if: failure()
        run: |
          echo "数据爬取失败于 $(date)"

  # 数据验证任务
  validate-data:
    needs: scrape-data
    runs-on: ubuntu-latest
    if: success()
    
    env:
      NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
    
    steps:
      - name: 检出代码
        uses: actions/checkout@v4
      
      - name: 设置 Python 环境
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: 安装依赖
        run: |
          cd python-scraper
          pip install -r requirements.txt
      
      - name: 验证数据质量
        run: |
          cd python-scraper
          python -c "
          import os
          from supabase import create_client

          supabase = create_client(
              os.environ['NEXT_PUBLIC_SUPABASE_URL'],
              os.environ['SUPABASE_SERVICE_ROLE_KEY']
          )

          # 检查今天新增的记录数
          from datetime import datetime, timedelta
          today = datetime.utcnow().strftime('%Y-%m-%d')
          
          permits = supabase.table('permits').select('id', count='exact').gte('created_at', today).execute()
          prices = supabase.table('price_records').select('id', count='exact').gte('created_at', today).execute()
          
          print(f'今日新增许可证记录: {permits.count}')
          print(f'今日新增价格记录: {prices.count}')
          
          if permits.count == 0:
              print('警告: 今日没有新增许可证记录')
          "

  # 生成报告
  generate-report:
    needs: validate-data
    runs-on: ubuntu-latest
    if: success()
    
    steps:
      - name: 生成每日报告
        run: |
          echo "## 数据更新报告" > report.md
          echo "更新时间: $(date)" >> report.md
          echo "" >> report.md
          echo "数据爬取和验证已完成。" >> report.md
      
      - name: 上传报告
        uses: actions/upload-artifact@v4
        with:
          name: daily-report-${{ github.run_id }}
          path: report.md
          retention-days: 30
